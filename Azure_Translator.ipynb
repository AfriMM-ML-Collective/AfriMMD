{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4zodp8Rp-48"
      },
      "outputs": [],
      "source": [
        "# Install the sentence-transformers library\n",
        "!pip install -q sentence-transformers\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import requests\n",
        "import uuid\n",
        "import json\n",
        "import time\n",
        "\n",
        "\n",
        "# Load the Flickr8k dataset containing the best captions into a Pandas DataFrame\n",
        "flickr8k = pd.read_csv('/content/drive/MyDrive/MMAD/flickr8k_best_captions.csv')\n",
        "\n",
        "# Display the first few rows of the dataset to ensure it loaded correctly\n",
        "flickr8k.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ekp5VyqQih3t"
      },
      "outputs": [],
      "source": [
        "# Your API subscription key for Microsoft Translator\n",
        "#key = {your Azure key} \n",
        "\n",
        "# Endpoint for the Microsoft Translator Text API\n",
        "endpoint = \"https://api.cognitive.microsofttranslator.com\"\n",
        "\n",
        "# The Azure region where your resource is located\n",
        "location = \"southafricanorth\"\n",
        "\n",
        "# API path for the translation service\n",
        "path = '/translate'\n",
        "\n",
        "# Construct the full URL for the API request\n",
        "constructed_url = endpoint + path\n",
        "\n",
        "# List of African language codes to which English text will be translated\n",
        "# African Languages:\n",
        "#   1. Afrikaans: af\n",
        "#   2. Amharic: am\n",
        "#   3. Kinyarwanda: rw\n",
        "#   4. Igbo: ig\n",
        "#   5. Hausa: ha\n",
        "#   6. Lingala: ln\n",
        "#   7. Luganda: lug\n",
        "#   8. Somali: so\n",
        "#   9. Zulu: zu\n",
        "#  10. Shona: sn\n",
        "#  11. Chichewa (Nyanja): nya\n",
        "#  12. Runyankore-Rukiga: run\n",
        "#  13. Sesotho: st\n",
        "#  14. Northern Sotho (Sepedi): nso\n",
        "#  15. Tswana: tn\n",
        "#  16. Xhosa: xh\n",
        "#  17. Yoruba: yo\n",
        "\n",
        "african_languages_code = [\n",
        "    \"af\", \"am\", \"rw\", \"ig\", \"ha\", \"ln\", \"lug\", \"so\",\n",
        "    \"zu\", \"sn\", \"nya\", \"run\", \"st\", \"nso\", \"tn\",\n",
        "    \"xh\", \"yo\"\n",
        "]\n",
        "\n",
        "# Parameters for the API request\n",
        "params = {\n",
        "    'api-version': '3.0',\n",
        "    'from': 'en',\n",
        "    'to': african_languages_code\n",
        "}\n",
        "\n",
        "# Headers for the API request\n",
        "headers = {\n",
        "    'Ocp-Apim-Subscription-Key': key,\n",
        "    'Ocp-Apim-Subscription-Region': location,\n",
        "    'Content-type': 'application/json',\n",
        "    'X-ClientTraceId': str(uuid.uuid4())\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8k7WjtOrWFP"
      },
      "outputs": [],
      "source": [
        "def get_batch_translate(params, body):\n",
        "    \"\"\"\n",
        "    Translates a list of texts in batches using the Microsoft Translator Text API.\n",
        "\n",
        "    Args:\n",
        "        params (dict): Parameters for the API request, including source and target languages.\n",
        "        body (list): A list of strings to be translated.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of translated texts.\n",
        "    \"\"\"\n",
        "\n",
        "    all_res = []  # List to store all translation results\n",
        "    current_batch = []  # List to store the current batch of texts to be translated\n",
        "    current_length = 0  # Counter for the total character length of the current batch\n",
        "    char_limit = 2500  # Character limit for each batch request as per API constraints\n",
        "\n",
        "    # Iterate over each text in the body list\n",
        "    for text in body:\n",
        "        text_length = len(str(text))  # Calculate the length of the current text\n",
        "\n",
        "        # Check if adding this text exceeds the character limit\n",
        "        if current_length + text_length <= char_limit:\n",
        "            current_batch.append(text)  # Add text to the current batch\n",
        "            current_length += text_length  # Update the total character length\n",
        "        else:\n",
        "            # Prepare the body for the API request with the current batch\n",
        "            body = [{'text': item} for item in current_batch]\n",
        "\n",
        "            # Make the API request for translation\n",
        "            request = requests.post(constructed_url, params=params, headers=headers, json=body)\n",
        "            response = request.json()\n",
        "\n",
        "            try:\n",
        "                # Extract the translated text from the response\n",
        "                reponse_text = [item[\"translations\"][0][\"text\"] for item in response]\n",
        "            except Exception as e:\n",
        "                # Handle any errors in parsing the response\n",
        "                print(\"Unable to parse response.\")\n",
        "                print(e, response)\n",
        "                break\n",
        "\n",
        "            # Append the translated text to the results list\n",
        "            all_res.append(reponse_text)\n",
        "\n",
        "            # Start a new batch with the current text\n",
        "            current_batch = [str(text)]\n",
        "            current_length = text_length\n",
        "\n",
        "    # Check if there are any remaining texts in the last batch\n",
        "    if current_batch:\n",
        "        body = [{'text': item} for item in current_batch]\n",
        "\n",
        "        # Make the final API request for the last batch\n",
        "        request = requests.post(constructed_url, params=params, headers=headers, json=body)\n",
        "        response = request.json()\n",
        "\n",
        "        try:\n",
        "            reponse_text = [item[\"translations\"][0][\"text\"] for item in response]\n",
        "        except Exception as e:\n",
        "            print(\"Unable to parse response.\")\n",
        "            print(e, response)\n",
        "\n",
        "        all_res.append(reponse_text)\n",
        "\n",
        "    # Flatten the list of lists into a single list of translated texts\n",
        "    return [element for row in all_res for element in row]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMS55U6STyp1"
      },
      "outputs": [],
      "source": [
        "def pprint(body):\n",
        "    \"\"\"\n",
        "    Pretty prints a JSON object or Python dictionary.\n",
        "\n",
        "    Args:\n",
        "        body (dict or list): The JSON object or Python dictionary to be printed.\n",
        "\n",
        "    Returns:\n",
        "        None: This function prints the formatted JSON to the console.\n",
        "    \"\"\"\n",
        "    # Use json.dumps to convert the dictionary to a pretty-printed JSON string\n",
        "    print(json.dumps(\n",
        "        body,\n",
        "        sort_keys=True,\n",
        "        ensure_ascii=False,\n",
        "        indent=4,\n",
        "        separators=(',', ': ')\n",
        "    ))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCcqN5c0XOvv"
      },
      "outputs": [],
      "source": [
        "# Create a DataFrame with the original English captions\n",
        "df = pd.DataFrame(\n",
        "    {'captions': flickr8k['best_caption']}\n",
        ")\n",
        "\n",
        "# Iterate over each African language code\n",
        "for code in african_languages_code:\n",
        "    # Update the 'to' parameter with the current language code\n",
        "    params['to'] = code\n",
        "\n",
        "    print(f\"Running translations of captions to {code}\\n......\")\n",
        "\n",
        "    # Batch translate the captions to the current language\n",
        "    response = get_batch_translate(params, body=flickr8k['best_caption'])\n",
        "\n",
        "    # Store the translated captions in a new column of the DataFrame\n",
        "    df[f'translated_{code}'] = response\n",
        "    print(f\"Translated English captions to {code}\")\n",
        "\n",
        "    # Pause for 2 minutes to avoid hitting the API rate limit\n",
        "    time.sleep(120)\n",
        "\n",
        "# Save the DataFrame with translated captions to a CSV file\n",
        "print(\"Translated English captions to 17 African languages.\")\n",
        "df.to_csv('/content/drive/MyDrive/MMAD/translated_african_captions.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoZ_JTbPihGf"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksxesQ_0-K0x"
      },
      "outputs": [],
      "source": [
        "# Set up parameters for back-translation to English\n",
        "params_en = {\n",
        "    'api-version': '3.0',\n",
        "    'from': '',  # Source language will be set dynamically in the loop\n",
        "    'to': 'en'  # Target language is always English\n",
        "}\n",
        "\n",
        "# Create a new DataFrame to store the original captions and back-translations\n",
        "new_df = pd.DataFrame(\n",
        "    {'captions': df['captions']}\n",
        ")\n",
        "\n",
        "# Iterate over each African language code to back-translate the captions\n",
        "for code in african_languages_code:\n",
        "    params_en['from'] = code  # Set the source language to the current language code\n",
        "\n",
        "    # Perform batch translation from the African language back to English\n",
        "    new_df[f'{code}_to_en'] = get_batch_translate(params_en, body=df[f\"translated_{code}\"])\n",
        "\n",
        "    # Pause for 2 minutes to avoid hitting the API rate limit\n",
        "    time.sleep(120)\n",
        "\n",
        "# Save the DataFrame with back-translated captions to a CSV file\n",
        "new_df.to_csv('/content/drive/MyDrive/MMAD/back_translated_african_captions.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cde6LA6NfIw8"
      },
      "outputs": [],
      "source": [
        "# Import necessary modules from the sentence-transformers library\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "# Load a pre-trained SentenceTransformer model\n",
        "# This model will be used to generate sentence embeddings\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2JARpwT3Iix"
      },
      "outputs": [],
      "source": [
        "# Create a DataFrame to store embeddings of the original English captions\n",
        "embd_df = pd.DataFrame(\n",
        "    {'captions': new_df['captions'].apply(lambda x: model.encode(x, convert_to_tensor=True))}\n",
        ")\n",
        "\n",
        "# Iterate over each African language code to generate embeddings for back-translated captions\n",
        "for code in african_languages_code:\n",
        "    print(f\"Embedding {code} captions\")\n",
        "\n",
        "    # Generate embeddings for the back-translated captions and store them in the DataFrame\n",
        "    embd_df[f'{code}_to_en'] = new_df[f'{code}_to_en'].apply(lambda x: model.encode(x, convert_to_tensor=True))\n",
        "\n",
        "# Save the DataFrame with caption embeddings to a CSV file\n",
        "embd_df.to_csv('/content/drive/MyDrive/MMAD/back_translated_african_captions_embd.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRMveD0wAxId"
      },
      "outputs": [],
      "source": [
        "# Initialize a dictionary to store cosine similarities\n",
        "cosine_similarities = {}\n",
        "cosine_similarities['captions'] = new_df['captions']  # Include the original captions for reference\n",
        "\n",
        "# Iterate over each African language code to compute cosine similarities\n",
        "for code in african_languages_code:\n",
        "    colName = f'{code}_sim'  # Name the column for the cosine similarity scores\n",
        "\n",
        "    cosine_similarities[colName] = []  # Initialize an empty list to store similarities for this language\n",
        "\n",
        "    # Calculate cosine similarity for each pair of original and back-translated embeddings\n",
        "    for i in range(len(embd_df)):\n",
        "        cos_sim = util.cos_sim(embd_df['captions'][i], embd_df[f'{code}_to_en'][i])  # Compute cosine similarity\n",
        "        cosine_similarities[colName].append(cos_sim.item())  # Append the similarity score as a float\n",
        "\n",
        "# Create a DataFrame from the cosine similarities dictionary\n",
        "cosine_df = pd.DataFrame(cosine_similarities)\n",
        "\n",
        "# Save the cosine similarity index to a CSV file\n",
        "cosine_df.to_csv('/content/drive/MyDrive/MMAD/back_translated_similarity_index.csv', index=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
